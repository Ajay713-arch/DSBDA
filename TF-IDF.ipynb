{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58539f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Thor eating pizza, Loki is eating pizza, Ironman ate pizza already\",\n",
    "    \"Apple is announcing new iphone tomorrow\",\n",
    "    \"Tesla is announcing new model-3 tomorrow\",\n",
    "    \"Google is announcing new pixel-6 tomorrow\",\n",
    "    \"Microsoft is announcing new surface tomorrow\",\n",
    "    \"Amazon is announcing new eco-dot tomorrow\",\n",
    "    \"I am eating biryani and you are eating grapes\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbfccbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create the vectorizer and fit the corpus and transform them accordingly\n",
    "v = TfidfVectorizer()\n",
    "v.fit(corpus)\n",
    "transform_output = v.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a0b712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thor': 25, 'eating': 10, 'pizza': 22, 'loki': 17, 'is': 16, 'ironman': 15, 'ate': 7, 'already': 0, 'apple': 5, 'announcing': 4, 'new': 20, 'iphone': 14, 'tomorrow': 26, 'tesla': 24, 'model': 19, 'google': 12, 'pixel': 21, 'microsoft': 18, 'surface': 23, 'amazon': 2, 'eco': 11, 'dot': 9, 'am': 1, 'biryani': 8, 'and': 3, 'you': 27, 'are': 6, 'grapes': 13}\n"
     ]
    }
   ],
   "source": [
    "#let's print the vocabulary\n",
    "\n",
    "print(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f90c906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_char_ngrams',\n",
       " '_char_wb_ngrams',\n",
       " '_check_feature_names',\n",
       " '_check_n_features',\n",
       " '_check_params',\n",
       " '_check_stop_words_consistency',\n",
       " '_check_vocabulary',\n",
       " '_count_vocab',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_limit_features',\n",
       " '_more_tags',\n",
       " '_parameter_constraints',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_sort_features',\n",
       " '_stop_words_id',\n",
       " '_tfidf',\n",
       " '_validate_data',\n",
       " '_validate_ngram_range',\n",
       " '_validate_params',\n",
       " '_validate_vocabulary',\n",
       " '_warn_for_unused_params',\n",
       " '_white_spaces',\n",
       " '_word_ngrams',\n",
       " 'analyzer',\n",
       " 'binary',\n",
       " 'build_analyzer',\n",
       " 'build_preprocessor',\n",
       " 'build_tokenizer',\n",
       " 'decode',\n",
       " 'decode_error',\n",
       " 'dtype',\n",
       " 'encoding',\n",
       " 'fit',\n",
       " 'fit_transform',\n",
       " 'fixed_vocabulary_',\n",
       " 'get_feature_names_out',\n",
       " 'get_params',\n",
       " 'get_stop_words',\n",
       " 'idf_',\n",
       " 'input',\n",
       " 'inverse_transform',\n",
       " 'lowercase',\n",
       " 'max_df',\n",
       " 'max_features',\n",
       " 'min_df',\n",
       " 'ngram_range',\n",
       " 'norm',\n",
       " 'preprocessor',\n",
       " 'set_params',\n",
       " 'smooth_idf',\n",
       " 'stop_words',\n",
       " 'stop_words_',\n",
       " 'strip_accents',\n",
       " 'sublinear_tf',\n",
       " 'token_pattern',\n",
       " 'tokenizer',\n",
       " 'transform',\n",
       " 'use_idf',\n",
       " 'vocabulary',\n",
       " 'vocabulary_']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Various functions that we can implement\n",
    "dir(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b611170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already : 2.386294361119891\n",
      "am : 2.386294361119891\n",
      "amazon : 2.386294361119891\n",
      "and : 2.386294361119891\n",
      "announcing : 1.2876820724517808\n",
      "apple : 2.386294361119891\n",
      "are : 2.386294361119891\n",
      "ate : 2.386294361119891\n",
      "biryani : 2.386294361119891\n",
      "dot : 2.386294361119891\n",
      "eating : 1.9808292530117262\n",
      "eco : 2.386294361119891\n",
      "google : 2.386294361119891\n",
      "grapes : 2.386294361119891\n",
      "iphone : 2.386294361119891\n",
      "ironman : 2.386294361119891\n",
      "is : 1.1335313926245225\n",
      "loki : 2.386294361119891\n",
      "microsoft : 2.386294361119891\n",
      "model : 2.386294361119891\n",
      "new : 1.2876820724517808\n",
      "pixel : 2.386294361119891\n",
      "pizza : 2.386294361119891\n",
      "surface : 2.386294361119891\n",
      "tesla : 2.386294361119891\n",
      "thor : 2.386294361119891\n",
      "tomorrow : 1.2876820724517808\n",
      "you : 2.386294361119891\n"
     ]
    }
   ],
   "source": [
    "#let's print the idf of each word:\n",
    "\n",
    "all_feature_names = v.get_feature_names_out()\n",
    "\n",
    "for word in all_feature_names:\n",
    "    \n",
    "    #let's get the index in the vocabulary\n",
    "    indx = v.vocabulary_.get(word)\n",
    "    \n",
    "    #get the score\n",
    "    idf_score = v.idf_[indx]\n",
    "    \n",
    "    print(f\"{word} : {idf_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29fae047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thor eating pizza, Loki is eating pizza, Ironman ate pizza already',\n",
       " 'Apple is announcing new iphone tomorrow']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To print first two sentences\n",
    "corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cc69fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 25)\t0.2426654728284301\n",
      "  (0, 22)\t0.7279964184852903\n",
      "  (0, 17)\t0.2426654728284301\n",
      "  (0, 16)\t0.11527032701364152\n",
      "  (0, 15)\t0.2426654728284301\n",
      "  (0, 10)\t0.40286636477562926\n",
      "  (0, 7)\t0.2426654728284301\n",
      "  (0, 0)\t0.2426654728284301\n",
      "  (1, 26)\t0.30652086071532464\n",
      "  (1, 20)\t0.30652086071532464\n",
      "  (1, 16)\t0.26982671076064085\n",
      "  (1, 14)\t0.5680354003049032\n",
      "  (1, 5)\t0.5680354003049032\n",
      "  (1, 4)\t0.30652086071532464\n",
      "  (2, 26)\t0.30652086071532464\n",
      "  (2, 24)\t0.5680354003049032\n",
      "  (2, 20)\t0.30652086071532464\n",
      "  (2, 19)\t0.5680354003049032\n",
      "  (2, 16)\t0.26982671076064085\n",
      "  (2, 4)\t0.30652086071532464\n",
      "  (3, 26)\t0.30652086071532464\n",
      "  (3, 21)\t0.5680354003049032\n",
      "  (3, 20)\t0.30652086071532464\n",
      "  (3, 16)\t0.26982671076064085\n",
      "  (3, 12)\t0.5680354003049032\n",
      "  (3, 4)\t0.30652086071532464\n",
      "  (4, 26)\t0.30652086071532464\n",
      "  (4, 23)\t0.5680354003049032\n",
      "  (4, 20)\t0.30652086071532464\n",
      "  (4, 18)\t0.5680354003049032\n",
      "  (4, 16)\t0.26982671076064085\n",
      "  (4, 4)\t0.30652086071532464\n",
      "  (5, 26)\t0.26652333217709795\n",
      "  (5, 20)\t0.26652333217709795\n",
      "  (5, 16)\t0.234617356529942\n",
      "  (5, 11)\t0.4939131624859276\n",
      "  (5, 9)\t0.4939131624859276\n",
      "  (5, 4)\t0.26652333217709795\n",
      "  (5, 2)\t0.4939131624859276\n",
      "  (6, 27)\t0.3379425688198216\n",
      "  (6, 13)\t0.3379425688198216\n",
      "  (6, 10)\t0.561042708781391\n",
      "  (6, 8)\t0.3379425688198216\n",
      "  (6, 6)\t0.3379425688198216\n",
      "  (6, 3)\t0.3379425688198216\n",
      "  (6, 1)\t0.3379425688198216\n"
     ]
    }
   ],
   "source": [
    "#let's print the transformed output from tf-idf\n",
    "print(transform_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a25099a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 25)\t0.2426654728284301\n",
      "  (0, 22)\t0.7279964184852903\n",
      "  (0, 17)\t0.2426654728284301\n",
      "  (0, 16)\t0.11527032701364152\n",
      "  (0, 15)\t0.2426654728284301\n",
      "  (0, 10)\t0.40286636477562926\n",
      "  (0, 7)\t0.2426654728284301\n",
      "  (0, 0)\t0.2426654728284301\n",
      "Thor eating pizza, Loki is eating pizza, Ironman ate pizza already\n",
      "  (0, 26)\t0.30652086071532464\n",
      "  (0, 20)\t0.30652086071532464\n",
      "  (0, 16)\t0.26982671076064085\n",
      "  (0, 14)\t0.5680354003049032\n",
      "  (0, 5)\t0.5680354003049032\n",
      "  (0, 4)\t0.30652086071532464\n",
      "Apple is announcing new iphone tomorrow\n",
      "  (0, 26)\t0.30652086071532464\n",
      "  (0, 24)\t0.5680354003049032\n",
      "  (0, 20)\t0.30652086071532464\n",
      "  (0, 19)\t0.5680354003049032\n",
      "  (0, 16)\t0.26982671076064085\n",
      "  (0, 4)\t0.30652086071532464\n",
      "Tesla is announcing new model-3 tomorrow\n",
      "  (0, 26)\t0.30652086071532464\n",
      "  (0, 21)\t0.5680354003049032\n",
      "  (0, 20)\t0.30652086071532464\n",
      "  (0, 16)\t0.26982671076064085\n",
      "  (0, 12)\t0.5680354003049032\n",
      "  (0, 4)\t0.30652086071532464\n",
      "Google is announcing new pixel-6 tomorrow\n",
      "  (0, 26)\t0.30652086071532464\n",
      "  (0, 23)\t0.5680354003049032\n",
      "  (0, 20)\t0.30652086071532464\n",
      "  (0, 18)\t0.5680354003049032\n",
      "  (0, 16)\t0.26982671076064085\n",
      "  (0, 4)\t0.30652086071532464\n",
      "Microsoft is announcing new surface tomorrow\n",
      "  (0, 26)\t0.26652333217709795\n",
      "  (0, 20)\t0.26652333217709795\n",
      "  (0, 16)\t0.234617356529942\n",
      "  (0, 11)\t0.4939131624859276\n",
      "  (0, 9)\t0.4939131624859276\n",
      "  (0, 4)\t0.26652333217709795\n",
      "  (0, 2)\t0.4939131624859276\n",
      "Amazon is announcing new eco-dot tomorrow\n",
      "  (0, 27)\t0.3379425688198216\n",
      "  (0, 13)\t0.3379425688198216\n",
      "  (0, 10)\t0.561042708781391\n",
      "  (0, 8)\t0.3379425688198216\n",
      "  (0, 6)\t0.3379425688198216\n",
      "  (0, 3)\t0.3379425688198216\n",
      "  (0, 1)\t0.3379425688198216\n",
      "I am eating biryani and you are eating grapes\n"
     ]
    }
   ],
   "source": [
    "for sentence,feature in zip(transform_output,corpus):\n",
    "    print(sentence)\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d715739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
